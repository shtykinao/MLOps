{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ClearML configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CLEARML_WEB_HOST=https://app.clear.ml\n",
      "env: CLEARML_API_HOST=https://api.clear.ml\n",
      "env: CLEARML_FILES_HOST=https://files.clear.ml\n",
      "env: CLEARML_API_ACCESS_KEY=V8AMVYWM2GZ50RRHALQUOQV7GMHHKI\n",
      "env: CLEARML_API_SECRET_KEY=89uJvM4pCoyT_CZxhYykQKx4fgukV2vh0n7F5yVKYj1Yn-etD6DpHN9oVV0riE2dQWE\n"
     ]
    }
   ],
   "source": [
    "%env CLEARML_WEB_HOST=https://app.clear.ml\n",
    "%env CLEARML_API_HOST=https://api.clear.ml\n",
    "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
    "%env CLEARML_API_ACCESS_KEY=V8AMVYWM2GZ50RRHALQUOQV7GMHHKI\n",
    "%env CLEARML_API_SECRET_KEY=89uJvM4pCoyT_CZxhYykQKx4fgukV2vh0n7F5yVKYj1Yn-etD6DpHN9oVV0riE2dQWE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from clearml import Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Простая сверточная нейронная сеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сверточная нейронная сеть с добавленным слоем Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetBN(nn.Module):  # Новое имя класса для модели с BN\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1) # <- Исправлено\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x)))) # BN после conv1 и ReLU\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x)))) # BN после conv2 и ReLU\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подгрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import ssl\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=1000, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка доступа, не обращай внимания, у меня не качался датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Проверка прав доступа к директории ./data\n",
      "Существует: True\n",
      "Доступ на запись: True\n",
      "Начинаем загрузку тренировочного набора данных...\n",
      "Files already downloaded and verified\n",
      "Тренировочный набор данных успешно загружен\n",
      "Начинаем загрузку тестового набора данных...\n",
      "Files already downloaded and verified\n",
      "Тестовый набор данных успешно загружен\n",
      "DataLoader'ы успешно созданы\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from torchvision import transforms\n",
    "import sys\n",
    "\n",
    "# Создаём директорию для данных\n",
    "data_dir = './data'\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "\n",
    "# Проверяем права доступа к директории\n",
    "print(f\"Проверка прав доступа к директории {data_dir}\")\n",
    "print(f\"Существует: {os.path.exists(data_dir)}\")\n",
    "print(f\"Доступ на запись: {os.access(data_dir, os.W_OK)}\")\n",
    "\n",
    "# Настройка трансформаций\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Попытка загрузки с подробным выводом ошибок\n",
    "try:\n",
    "    print(\"Начинаем загрузку тренировочного набора данных...\")\n",
    "    trainset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, \n",
    "        train=True, \n",
    "        download=True, \n",
    "        transform=transform\n",
    "    )\n",
    "    print(\"Тренировочный набор данных успешно загружен\")\n",
    "    \n",
    "    print(\"Начинаем загрузку тестового набора данных...\")\n",
    "    testset = torchvision.datasets.CIFAR10(\n",
    "        root=data_dir, \n",
    "        train=False, \n",
    "        download=True, \n",
    "        transform=transform\n",
    "    )\n",
    "    print(\"Тестовый набор данных успешно загружен\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Произошла ошибка при загрузке данных: {e}\")\n",
    "    print(f\"Тип ошибки: {type(e).__name__}\")\n",
    "    print(\"Полный traceback:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Если загрузка успешна, создаём DataLoader\n",
    "try:\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        trainset, \n",
    "        batch_size=64, \n",
    "        shuffle=True, \n",
    "        num_workers=2\n",
    "    )\n",
    "    testloader = torch.utils.data.DataLoader(\n",
    "        testset, \n",
    "        batch_size=1000, \n",
    "        shuffle=False, \n",
    "        num_workers=2\n",
    "    )\n",
    "    print(\"DataLoader'ы успешно созданы\")\n",
    "except NameError:\n",
    "    print(\"DataLoader'ы не созданы, так как возникла ошибка при загрузке данных\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка при создании DataLoader: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Описание train и test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, epochs, trainloader, series='Baseline'):\n",
    "    criterion = nn.CrossEntropyLoss() # Используем CrossEntropyLoss\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        for batch_idx, (data, target) in enumerate(trainloader):\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        train_accuracy = test(model, testloader, \"Train\", series, epoch)\n",
    "            # task.get_logger().report_scalar(title='Accuracy', series=series, value=train_accuracy, iteration=epoch)\n",
    "        task.get_logger().report_scalar(title='Loss', series=series, value=loss.item(), iteration=epoch)\n",
    "        print(f'Train Epoch: {epoch}\\tLoss: {loss.item():.6f}\\tAccuracy: {train_accuracy:.2f}%')\n",
    "\n",
    "\n",
    "def test(model, dataloader, data_type=\"Test\", series=\"Baseline\", epoch=0):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    criterion = nn.CrossEntropyLoss(reduction='sum') # Используем CrossEntropyLoss\n",
    "    with torch.no_grad():\n",
    "        for data, target in dataloader:\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(dataloader.dataset)\n",
    "    accuracy = 100. * correct / len(dataloader.dataset)\n",
    "    task.get_logger().report_scalar(title=f'{data_type} Loss', series=f'{data_type} {series}', value=test_loss, iteration=epoch)\n",
    "    print(f'\\n{data_type} set: Average loss: {test_loss:.4f}, Accuracy: {correct}/{len(dataloader.dataset)} ({accuracy:.0f}%)\\n')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Дефолтный запуск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Testing\n",
      "\n",
      "Train set: Average loss: 1.2125, Accuracy: 5771/10000 (58%)\n",
      "\n",
      "Train Epoch: 1\tLoss: 1.063691\tAccuracy: 57.71%\n",
      "\n",
      "Train set: Average loss: 0.9699, Accuracy: 6583/10000 (66%)\n",
      "\n",
      "Train Epoch: 2\tLoss: 0.877451\tAccuracy: 65.83%\n",
      "\n",
      "Train set: Average loss: 0.8925, Accuracy: 6862/10000 (69%)\n",
      "\n",
      "Train Epoch: 3\tLoss: 0.748714\tAccuracy: 68.62%\n",
      "\n",
      "Train set: Average loss: 0.8322, Accuracy: 7137/10000 (71%)\n",
      "\n",
      "Train Epoch: 4\tLoss: 0.729443\tAccuracy: 71.37%\n",
      "\n",
      "Train set: Average loss: 0.8608, Accuracy: 7205/10000 (72%)\n",
      "\n",
      "Train Epoch: 5\tLoss: 0.520230\tAccuracy: 72.05%\n",
      "\n",
      "Test set: Average loss: 0.8608, Accuracy: 7205/10000 (72%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task = Task.init(project_name=\"MLOps_Lab\", task_name=\"CIFAR-10_Experiment_Default\", )\n",
    "\n",
    "task.get_logger().report_text('Default Testing')\n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "train(model, optimizer, epochs=5, trainloader=trainloader)\n",
    "test_accuracy = test(model, testloader, series='Baseline')\n",
    "\n",
    "task.get_logger().report_scalar(title='Test Accuracy', series='Baseline', value=test_accuracy, iteration=0)\n",
    "\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Гипотеза 1: Увеличение количества эпох"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not read Jupyter Notebook: No module named 'nbconvert'\n",
      "Please install nbconvert using \"pip install nbconvert\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=00d7e93f778a4dc99cf066def1cc00e3\n",
      "ClearML results page: https://app.clear.ml/projects/dbc17164c4b14c1ca302acb9aa34ebd2/experiments/00d7e93f778a4dc99cf066def1cc00e3/output/log\n",
      "Testing Hypothesis 1\n",
      "ClearML Monitor: GPU monitoring failed getting GPU reading, switching off GPU monitoring\n",
      "\n",
      "Train set: Average loss: 1.1761, Accuracy: 5879/10000 (59%)\n",
      "\n",
      "Train Epoch: 1\tLoss: 1.050064\tAccuracy: 58.79%\n",
      "\n",
      "Train set: Average loss: 1.0105, Accuracy: 6517/10000 (65%)\n",
      "\n",
      "Train Epoch: 2\tLoss: 0.789073\tAccuracy: 65.17%\n",
      "\n",
      "Train set: Average loss: 0.8870, Accuracy: 7023/10000 (70%)\n",
      "\n",
      "Train Epoch: 3\tLoss: 0.766194\tAccuracy: 70.23%\n",
      "\n",
      "Train set: Average loss: 0.8498, Accuracy: 7151/10000 (72%)\n",
      "\n",
      "Train Epoch: 4\tLoss: 0.673228\tAccuracy: 71.51%\n",
      "\n",
      "Train set: Average loss: 0.8487, Accuracy: 7202/10000 (72%)\n",
      "\n",
      "Train Epoch: 5\tLoss: 0.279211\tAccuracy: 72.02%\n",
      "\n",
      "Train set: Average loss: 0.9629, Accuracy: 7128/10000 (71%)\n",
      "\n",
      "Train Epoch: 6\tLoss: 0.653534\tAccuracy: 71.28%\n",
      "\n",
      "Train set: Average loss: 1.0690, Accuracy: 7251/10000 (73%)\n",
      "\n",
      "Train Epoch: 7\tLoss: 0.241029\tAccuracy: 72.51%\n",
      "\n",
      "Train set: Average loss: 1.1770, Accuracy: 7314/10000 (73%)\n",
      "\n",
      "Train Epoch: 8\tLoss: 0.037565\tAccuracy: 73.14%\n",
      "\n",
      "Train set: Average loss: 1.3776, Accuracy: 7170/10000 (72%)\n",
      "\n",
      "Train Epoch: 9\tLoss: 0.091004\tAccuracy: 71.70%\n",
      "\n",
      "Train set: Average loss: 1.4152, Accuracy: 7138/10000 (71%)\n",
      "\n",
      "Train Epoch: 10\tLoss: 0.080964\tAccuracy: 71.38%\n",
      "\n",
      "Test set: Average loss: 1.4152, Accuracy: 7138/10000 (71%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task = Task.init(project_name=\"MLOps_Lab\", task_name=\"CIFAR-10_Experiment_Hypothesis_1\")\n",
    "\n",
    "task.get_logger().report_text('Testing Hypothesis 1')\n",
    "\n",
    "model = Net()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "train(model, optimizer, epochs=10, trainloader=trainloader, series='MoreEpoch')\n",
    "test_accuracy = test(model, testloader, series='MoreEpoch')\n",
    "\n",
    "task.get_logger().report_scalar(title='Test Accuracy', series='MoreEpoch', value=test_accuracy, iteration=0)\n",
    "\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Гипотеза 2: Batch Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not read Jupyter Notebook: No module named 'nbconvert'\n",
      "Please install nbconvert using \"pip install nbconvert\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=0aebb65c63414295a735dfcd392ee3fc\n",
      "ClearML results page: https://app.clear.ml/projects/dbc17164c4b14c1ca302acb9aa34ebd2/experiments/0aebb65c63414295a735dfcd392ee3fc/output/log\n",
      "Testing Hypothesis 2\n",
      "ClearML Monitor: GPU monitoring failed getting GPU reading, switching off GPU monitoring\n",
      "\n",
      "Train set: Average loss: 1.0101, Accuracy: 6431/10000 (64%)\n",
      "\n",
      "Train Epoch: 1\tLoss: 0.822444\tAccuracy: 64.31%\n",
      "\n",
      "Train set: Average loss: 0.9907, Accuracy: 6648/10000 (66%)\n",
      "\n",
      "Train Epoch: 2\tLoss: 0.600236\tAccuracy: 66.48%\n",
      "\n",
      "Train set: Average loss: 0.8166, Accuracy: 7186/10000 (72%)\n",
      "\n",
      "Train Epoch: 3\tLoss: 0.725312\tAccuracy: 71.86%\n",
      "\n",
      "Train set: Average loss: 0.7718, Accuracy: 7357/10000 (74%)\n",
      "\n",
      "Train Epoch: 4\tLoss: 0.629568\tAccuracy: 73.57%\n",
      "\n",
      "Train set: Average loss: 0.8365, Accuracy: 7255/10000 (73%)\n",
      "\n",
      "Train Epoch: 5\tLoss: 0.439213\tAccuracy: 72.55%\n",
      "\n",
      "Test set: Average loss: 0.8365, Accuracy: 7255/10000 (73%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task = Task.init(project_name=\"MLOps_Lab\", task_name=\"CIFAR-10_Experiment_Hypothesis_2\")\n",
    "\n",
    "task.get_logger().report_text('Testing Hypothesis 2')\n",
    "\n",
    "model_bn = NetBN()  # Модель с BN\n",
    "optimizer_bn = optim.SGD(model_bn.parameters(), lr=0.01, momentum=0.9)\n",
    "train(model_bn, optimizer_bn, epochs=5, trainloader=trainloader, series='Batch Normalization')\n",
    "test_accuracy_bn = test(model_bn, testloader, series='Batch Normalization')\n",
    "\n",
    "task.get_logger().report_scalar(title='Test Accuracy', series='Batch Normalization', value=test_accuracy, iteration=0)\n",
    "\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Гипотеза 3: Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Hypothesis 3\n",
      "\n",
      "Train set: Average loss: 1.0298, Accuracy: 6410/10000 (64%)\n",
      "\n",
      "Train Epoch: 1\tLoss: 0.811755\tAccuracy: 64.10%\n",
      "\n",
      "Train set: Average loss: 0.9522, Accuracy: 6712/10000 (67%)\n",
      "\n",
      "Train Epoch: 2\tLoss: 0.710793\tAccuracy: 67.12%\n",
      "\n",
      "Train set: Average loss: 0.8693, Accuracy: 7024/10000 (70%)\n",
      "\n",
      "Train Epoch: 3\tLoss: 0.541665\tAccuracy: 70.24%\n",
      "\n",
      "Train set: Average loss: 0.8809, Accuracy: 7010/10000 (70%)\n",
      "\n",
      "Train Epoch: 4\tLoss: 0.556091\tAccuracy: 70.10%\n",
      "\n",
      "Train set: Average loss: 0.8471, Accuracy: 7224/10000 (72%)\n",
      "\n",
      "Train Epoch: 5\tLoss: 0.270223\tAccuracy: 72.24%\n",
      "\n",
      "Test set: Average loss: 0.8471, Accuracy: 7224/10000 (72%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task = Task.init(project_name=\"MLOps_Lab\", task_name=\"CIFAR-10_Experiment_Hypothesis_3\")\n",
    "\n",
    "task.get_logger().report_text('Testing Hypothesis 3')\n",
    "\n",
    "model3 = Net()\n",
    "optimizer3 = optim.Adam(model3.parameters(), lr=0.001)\n",
    "train(model3, optimizer3, epochs=5, trainloader=trainloader, series='Adam Optimizer')\n",
    "accuracy3 = test(model3, testloader, series='Adam Optimizer')\n",
    "\n",
    "task.get_logger().report_scalar(title='Test Accuracy', series='Adam Optimizer', value=accuracy3, iteration=0)\n",
    "\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AllIn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClearML Task: created new task id=e64013c1d55b40249ccdfb2fa7bfbca5\n",
      "ClearML results page: https://app.clear.ml/projects/dbc17164c4b14c1ca302acb9aa34ebd2/experiments/e64013c1d55b40249ccdfb2fa7bfbca5/output/log\n",
      "Testing Hypothesis AllIn\n",
      "ClearML Monitor: GPU monitoring failed getting GPU reading, switching off GPU monitoring\n",
      "\n",
      "Train set: Average loss: 1.1181, Accuracy: 6141/10000 (61%)\n",
      "\n",
      "Train Epoch: 1\tLoss: 0.709194\tAccuracy: 61.41%\n",
      "\n",
      "Train set: Average loss: 1.1807, Accuracy: 5977/10000 (60%)\n",
      "\n",
      "Train Epoch: 2\tLoss: 0.747219\tAccuracy: 59.77%\n",
      "\n",
      "Train set: Average loss: 0.9300, Accuracy: 6918/10000 (69%)\n",
      "\n",
      "Train Epoch: 3\tLoss: 0.948523\tAccuracy: 69.18%\n",
      "\n",
      "Train set: Average loss: 0.7877, Accuracy: 7234/10000 (72%)\n",
      "\n",
      "Train Epoch: 4\tLoss: 1.053416\tAccuracy: 72.34%\n",
      "\n",
      "Train set: Average loss: 0.8583, Accuracy: 7139/10000 (71%)\n",
      "\n",
      "Train Epoch: 5\tLoss: 0.277816\tAccuracy: 71.39%\n",
      "\n",
      "Train set: Average loss: 0.7928, Accuracy: 7357/10000 (74%)\n",
      "\n",
      "Train Epoch: 6\tLoss: 0.846929\tAccuracy: 73.57%\n",
      "\n",
      "Train set: Average loss: 0.8784, Accuracy: 7304/10000 (73%)\n",
      "\n",
      "Train Epoch: 7\tLoss: 0.354722\tAccuracy: 73.04%\n",
      "\n",
      "Train set: Average loss: 0.8399, Accuracy: 7430/10000 (74%)\n",
      "\n",
      "Train Epoch: 8\tLoss: 0.210504\tAccuracy: 74.30%\n",
      "\n",
      "Train set: Average loss: 0.9216, Accuracy: 7361/10000 (74%)\n",
      "\n",
      "Train Epoch: 9\tLoss: 0.461314\tAccuracy: 73.61%\n",
      "\n",
      "Train set: Average loss: 1.1137, Accuracy: 7163/10000 (72%)\n",
      "\n",
      "Train Epoch: 10\tLoss: 0.066984\tAccuracy: 71.63%\n",
      "\n",
      "Test set: Average loss: 1.1137, Accuracy: 7163/10000 (72%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "task = Task.init(project_name=\"MLOps_Lab\", task_name=\"CIFAR-10_Experiment_Final\")\n",
    "\n",
    "task.get_logger().report_text('Testing Hypothesis AllIn')\n",
    "\n",
    "model3 = NetBN()\n",
    "optimizer3 = optim.Adam(model3.parameters(), lr=0.001)\n",
    "train(model3, optimizer3, epochs=10, trainloader=trainloader, series='AllIn')\n",
    "accuracy3 = test(model3, testloader, series='AllIn')\n",
    "\n",
    "task.get_logger().report_scalar(title='Test Accuracy', series='AllIn', value=accuracy3, iteration=0)\n",
    "\n",
    "task.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "task.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
